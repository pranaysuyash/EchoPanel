# EchoPanel Server Configuration (Optional)
# Copy this to .env to override defaults.

# üêû Debug Mode (0 or 1)
# ECHOPANEL_DEBUG=1

# üß† ASR Model Selection
# Options: tiny, base, small, medium, large-v3-turbo (default), large-v3
# ECHOPANEL_WHISPER_MODEL=large-v3-turbo

# ‚öôÔ∏è Computation Settings
# Device: "cpu", "cuda", "mps" (auto-detected)
# ECHOPANEL_WHISPER_DEVICE=auto
# Compute Type: "int8", "float16", "float32"
# ECHOPANEL_WHISPER_COMPUTE=int8

# üó£Ô∏è Diarization (Speaker Labeling)
# Set to 1 to enable. Requires Valid HuggingFace Token.
# ECHOPANEL_DIARIZATION=0
# ECHOPANEL_HF_TOKEN=hf_...
# ECHOPANEL_DIARIZATION_MAX_SECONDS=1800 (Max buffer size)

# ü§ñ ASR Provider Selection
# Options:
# - "faster_whisper" (default, local; CPU on macOS)
# - "whisper_cpp" (local; whisper.cpp via `whisper-cli`, Metal on Apple Silicon)
# - "mlx_whisper" (local; mlx-whisper Python, Metal on Apple Silicon)
# - "voxtral_realtime" (local; voxtral.c streaming mode, huge model)
# - "onnx_whisper" (local; ONNX Runtime + CoreML EP) NOTE: provider scaffold exists but inference is not implemented yet
# ECHOPANEL_ASR_PROVIDER=faster_whisper

# üó£Ô∏è Voxtral Realtime (local, open-source, requires voxtral.c + 8.9GB model)
# ECHOPANEL_VOXTRAL_BIN=../voxtral.c/voxtral
# ECHOPANEL_VOXTRAL_MODEL=../voxtral.c/voxtral-model

# üó£Ô∏è whisper.cpp (local; required for ECHOPANEL_ASR_PROVIDER=whisper_cpp)
# WHISPER_CPP_BIN=whisper-cli
# WHISPER_CPP_MODEL_DIR=~/.cache/whisper

# üó£Ô∏è ONNX Whisper (local; required for ECHOPANEL_ASR_PROVIDER=onnx_whisper)
# WHISPER_ONNX_MODEL_DIR=~/.cache/whisper-onnx

# üé§ Audio Processing
# Chunk size for ASR processing in seconds
# ECHOPANEL_ASR_CHUNK_SECONDS=4
# VAD (Voice Activity Detection): 0 (off) or 1 (on)
# ECHOPANEL_ASR_VAD=0

# üß† LLM Analysis (planned)
# NOTE: as of 2026-02-14 this repo documents an opt-in LLM path, but the provider integration
# is not yet wired into the running pipeline.
# ECHOPANEL_LLM_PROVIDER=none  # none | openai | ollama
