# MOD-005 Fixed-Size Chunked Batch Inference

## Summary

- Origin: Code-only
- Status: Implemented
- 1â€“2 sentence description: Processes audio in fixed-size chunks for streaming ASR, maintaining accurate timestamps and preventing hallucinations from small final buffers.
- Boundaries crossed: Process (chunking logic), Model (batch inference), Network (streaming output)
- Primary components (coarse list): ASRProvider.transcribe_stream, chunk size configuration, inference locking

## Triggers and Preconditions

- Triggers: Audio data received, buffer reaches chunk_bytes threshold
- Preconditions: Model loaded, sample rate known, chunk_seconds configured

## Sequence (Happy Path)

1. **Chunk parameter calculation**
   - Determine chunk_bytes from sample_rate _ chunk_seconds _ 2
   - Initialize buffer and processed_samples counter
   - Evidence:
     - Code: provider_faster_whisper.py:120-126
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:737

2. **Audio chunk accumulation**
   - Extend buffer with incoming PCM bytes
   - Evidence:
     - Code: provider_faster_whisper.py:144-145
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:742

3. **Chunk size threshold check**
   - While buffer >= chunk_bytes, process chunks
   - Evidence:
     - Code: provider_faster_whisper.py:148
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:745

4. **Fixed-size chunk extraction**
   - Extract exactly chunk_bytes from buffer
   - Calculate timestamps based on processed_samples
   - Evidence:
     - Code: provider_faster_whisper.py:148-157
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:748

5. **Audio format conversion**
   - Convert int16 PCM to float32 numpy array
   - Evidence:
     - Code: provider_faster_whisper.py:161
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:757

6. **Thread-safe inference execution**
   - Acquire \_infer_lock to serialize model access
   - Run transcribe() in thread pool
   - Evidence:
     - Code: provider_faster_whisper.py:163-174
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:760

7. **Confidence calculation**
   - Convert avg_logprob to confidence score
   - Evidence:
     - Code: provider_faster_whisper.py:192-196
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:768

8. **Segment emission**
   - Yield ASRSegment with adjusted timestamps
   - Evidence:
     - Code: provider_faster_whisper.py:201-209
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:773

9. **Final buffer processing**
   - Process remaining audio at stream end
   - Skip small buffers to prevent hallucinations
   - Evidence:
     - Code: provider_faster_whisper.py:211-276
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:778

## Inputs and Outputs

- Inputs:
  - PCM audio stream (async iterator of bytes)
  - Sample rate, chunk_seconds config
- Outputs:
  - ASRSegment stream with text, timestamps, confidence
- Side effects: Model inference lock held, buffer memory usage, timestamp accumulation

## Failure Modes

- Failure: Inference lock deadlock
  - Where detected: \_infer_lock not released
  - Current handling: No timeout, blocks indefinitely
  - User-visible outcome: All inference stops, session hangs
  - Evidence: provider_faster_whisper.py:166-174

- Failure: Model inference exception
  - Where detected: transcribe() call fails
  - Current handling: Exception propagates, stream ends
  - User-visible outcome: Partial transcription, session may continue
  - Evidence: provider_faster_whisper.py:163-174

- Failure: Buffer corruption
  - Where detected: np.frombuffer() fails on invalid data
  - Current handling: Exception raised
  - User-visible outcome: Stream fails with error
  - Evidence: provider_faster_whisper.py:161

- Failure: Small final chunk hallucination
  - Where detected: Final buffer < min_final_bytes
  - Current handling: Skip chunk, log warning
  - User-visible outcome: Some final audio not transcribed
  - Evidence: provider_faster_whisper.py:225-228

- Failure: Low energy final chunk
  - Where detected: audio_energy < 0.01 threshold
  - Current handling: Skip chunk to avoid silence transcription
  - User-visible outcome: Silent endings not transcribed
  - Evidence: provider_faster_whisper.py:231-234

- Failure: Timestamp drift
  - Where detected: processed_samples counter becomes inaccurate
  - Current handling: None - timestamps based on accumulation
  - User-visible outcome: Incorrect segment timing
  - Evidence: provider_faster_whisper.py:125,154-157

## Data and Storage

- What data is produced/consumed: Audio chunks, segment metadata, confidence scores
- Where it is stored: In-memory buffer, processed_samples counter
- Retention / deletion controls: Buffer cleared per chunk, counters reset per stream

## Observability

- Logs/events emitted: Chunk processing, inference timing, buffer skips
- Correlation IDs / session IDs: None (per-stream)
- Metrics/traces: Inference time, chunk count, confidence distribution
- Evidence: provider_faster_whisper.py logging

## Open Questions and Follow-up Tasks

- Questions: How to handle variable-length final chunks optimally?
- Documentation gaps: Chunk size impact on latency vs accuracy, lock contention scenarios
- Test gaps: Chunk boundary transcription accuracy, final buffer handling edge cases
