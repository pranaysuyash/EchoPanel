# MOD-003 Eager Load + 3-Phase Warmup (Load → Warm → Ready)

## Summary
- Origin: Code-only
- Status: Implemented
- 1–2 sentence description: Performs eager model loading and multi-level warmup at server startup to ensure ASR is ready for immediate inference with optimal performance.
- Boundaries crossed: Process (async initialization), Model (loading and inference), Storage (model files)
- Primary components (coarse list): ModelManager, model_preloader.py, ASRProvider

## Triggers and Preconditions
- Triggers: Server startup (FastAPI lifespan), initialize_model_at_startup() call
- Preconditions: Provider selected and available, sufficient RAM for model

## Sequence (Happy Path)
1. **ModelManager singleton acquisition**
   - Get or create ModelManager instance for provider/config combination
   - Evidence:
     - Code: model_preloader.py:355-368
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:315

2. **Concurrent initialization check**
   - If already READY, return immediately
   - If LOADING, wait for other task to complete
   - Evidence:
     - Code: model_preloader.py:142-153
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:318

3. **State transition to LOADING**
   - Set _state = ModelState.LOADING
   - Evidence:
     - Code: model_preloader.py:155
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:324

4. **Phase 1: Model loading**
   - Get provider instance from registry
   - Verify provider availability
   - Evidence:
     - Code: model_preloader.py:158-166, 197-218
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:327

5. **State transition to WARMING_UP**
   - Set _state = ModelState.WARMING_UP
   - Evidence:
     - Code: model_preloader.py:170-171
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:342

6. **Phase 2: Warmup execution**
   - Run configured warmup levels (1, 2, 3)
   - Evidence:
     - Code: model_preloader.py:173-179
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:345

7. **Level 1: Basic model validation**
   - Ensure model loads without errors
   - Evidence:
     - Code: model_preloader.py:227-233
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:349

8. **Level 2: Single inference warmup**
   - Run one full inference to populate caches
   - Enforce minimum duration to ensure cache effectiveness
   - Evidence:
     - Code: model_preloader.py:234-253
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:352

9. **Level 3: Stress test warmup**
   - Run multiple inferences to fully warm up
   - Enforce minimum duration for comprehensive warmup
   - Evidence:
     - Code: model_preloader.py:256-276
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:367

10. **State transition to READY**
    - Set _state = ModelState.READY
    - Set _ready_event to unblock waiters
    - Evidence:
      - Code: model_preloader.py:182-184
      - Docs: docs/audit/asr-model-lifecycle-20260211.md:382

## Inputs and Outputs
- Inputs:
  - Provider name and ASRConfig
  - WarmupConfig (levels, durations, iterations)
- Outputs:
  - Boolean success flag
  - ModelHealth with timing metrics
- Side effects: Model loaded in memory, GPU/CPU caches populated, provider instance created

## Failure Modes
- Failure: Provider not available
  - Where detected: is_available check in _load_model
  - Current handling: Raises RuntimeError
  - User-visible outcome: Initialization fails, server startup blocked
  - Evidence: model_preloader.py:209-210

- Failure: Model loading timeout
  - Where detected: initialize() timeout parameter (300s default)
  - Current handling: Returns False, logs timeout
  - User-visible outcome: Model not ready, ASR unavailable
  - Evidence: model_preloader.py:131

- Failure: Concurrent initialization timeout
  - Where detected: wait_for_ready() timeout while waiting for other task
  - Current handling: Returns False, allows duplicate initialization
  - User-visible outcome: Temporary failure, may retry
  - Evidence: model_preloader.py:150-153

- Failure: Warmup inference fails
  - Where detected: Exception in _warmup() during transcribe_stream
  - Current handling: Propagates exception, initialization fails
  - User-visible outcome: Model not ready
  - Evidence: model_preloader.py:220-276

- Failure: Insufficient memory for model
  - Where detected: OOM during provider instantiation
  - Current handling: RuntimeError, no recovery
  - User-visible outcome: Server crash or initialization failure
  - Evidence: model_preloader.py:189-195

- Failure: Model file corrupted/missing
  - Where detected: Provider load_model() fails
  - Current handling: Exception propagates
  - User-visible outcome: Permanent ASR unavailability
  - Evidence: provider_whisper_cpp.py:128-129

## Data and Storage
- What data is produced/consumed: Model weights, warmup audio samples, timing metrics
- Where it is stored: GPU/CPU memory (model), in-memory state (_state, _ready_event)
- Retention / deletion controls: Model manager exposes explicit unload and runs it during FastAPI shutdown; provider cache entry is evicted so next startup/session reloads cleanly

## Observability
- Logs/events emitted: Phase transitions, timing metrics, warmup progress
- Correlation IDs / session IDs: None (startup only)
- Metrics/traces: Load time, warmup time, health status via /health endpoint
- Evidence: model_preloader.py logging, main.py:101-156

## Open Questions and Follow-up Tasks
- Questions: What happens if model becomes unavailable during runtime?
- Documentation gaps: Warmup level configuration rationale, memory usage patterns
- Test gaps: Warmup effectiveness measurement, concurrent initialization race conditions
