# MOD-004 Lazy Model Loading on First Inference

## Summary

- Origin: Code-only
- Status: Implemented
- 1â€“2 sentence description: Loads ASR models on-demand during the first transcription request, with automatic device selection and fallback handling.
- Boundaries crossed: Process (lazy initialization), Model (runtime loading), Storage (model files)
- Primary components (coarse list): ASRProvider implementations, device detection, model caching

## Triggers and Preconditions

- Triggers: First transcribe_stream() call when model not loaded
- Preconditions: Provider instance exists, model files available, sufficient resources

## Sequence (Happy Path)

1. **Transcription request initiation**
   - transcribe_stream() called on provider
   - Evidence:
     - Code: provider_faster_whisper.py:113-118
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:509

2. **Model availability check**
   - Call \_get_model() to check if model loaded
   - Evidence:
     - Code: provider_faster_whisper.py:130, provider_whisper_cpp.py:207
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:512

3. **Lazy loading trigger**
   - If \_model is None, initiate loading
   - Evidence:
     - Code: provider_faster_whisper.py:80-84, provider_whisper_cpp.py:133-134
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:515

4. **Device and compute type determination**
   - Apply auto device selection (Metal/CPU for macOS)
   - Adjust compute_type for device compatibility
   - Evidence:
     - Code: provider_faster_whisper.py:85-99
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:518

5. **Model instantiation**
   - Create WhisperModel or Model instance
   - Handle Metal vs CPU configuration
   - Evidence:
     - Code: provider_faster_whisper.py:101-111, provider_whisper_cpp.py:139-173
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:532

6. **Model caching**
   - Store loaded model in \_model attribute
   - Update health metrics and timestamps
   - Evidence:
     - Code: provider_faster_whisper.py:46, provider_whisper_cpp.py:171
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:547

7. **Transcription continuation**
   - Proceed with inference using loaded model
   - Evidence:
     - Code: provider_faster_whisper.py:119-157
     - Docs: docs/audit/asr-model-lifecycle-20260211.md:509

## Inputs and Outputs

- Inputs:
  - PCM audio stream
  - ASRConfig (model_name, device, compute_type)
- Outputs:
  - ASR segment stream
  - Loaded model instance (cached)
- Side effects: Model loaded in memory, GPU/CPU resources allocated, first inference latency increased

## Failure Modes

- Failure: Model file not found
  - Where detected: WhisperModel/Model constructor
  - Current handling: Exception raised, health updated
  - User-visible outcome: Transcription fails with error segment
  - Evidence: provider_faster_whisper.py:107-111

- Failure: Insufficient GPU memory
  - Where detected: CUDA/Metal allocation failure
  - Current handling: Falls back to CPU automatically
  - User-visible outcome: Slower performance but continues working
  - Evidence: provider_faster_whisper.py:104

- Failure: Unsupported compute type for device
  - Where detected: Device/compute validation in \_get_model
  - Current handling: Force int8 for CPU, log warning
  - User-visible outcome: Sub-optimal but working configuration
  - Evidence: provider_faster_whisper.py:95-99

- Failure: Model loading timeout
  - Where detected: Exception during model instantiation
  - Current handling: Health metrics updated, error logged
  - User-visible outcome: Provider becomes unavailable
  - Evidence: provider_whisper_cpp.py:165-173

- Failure: PyTorch/dependencies missing
  - Where detected: Import errors during loading
  - Current handling: is_available returns False
  - User-visible outcome: Provider not selectable
  - Evidence: provider_faster_whisper.py:80-84

- Failure: Metal not available on Apple Silicon
  - Where detected: torch.backends.mps.is_available() check
  - Current handling: Falls back to CPU
  - User-visible outcome: CPU inference (slower)
  - Evidence: provider_whisper_cpp.py:143-148

## Data and Storage

- What data is produced/consumed: Model weights, configuration parameters
- Where it is stored: GPU/CPU memory (\_model attribute), filesystem (model files)
- Retention / deletion controls: Model cached until provider instance destroyed

## Observability

- Logs/events emitted: Loading progress, device selection, load timing
- Correlation IDs / session IDs: None (per-provider)
- Metrics/traces: Load time, device used, health status
- Evidence: provider_faster_whisper.py logging, /health endpoint

## Open Questions and Follow-up Tasks

- Questions: How to handle model unloading or reloading with different configs?
- Documentation gaps: Device selection priority order, compute type compatibility matrix
- Test gaps: Model loading failure recovery, device fallback effectiveness
