# INT-010 Incremental Analysis Update Flow

## Summary

- Origin: Mixed
- Status: Implemented
- Applies sliding window filtering (last 10 minutes) to transcript before analysis, but performs full re-analysis on each cycle rather than true incremental updates.
- Boundaries crossed: Process (analysis computations) / Memory (transcript filtering)
- Primary components (coarse list): \_filter_window function, analysis_stream.py functions

## Triggers and Preconditions

- Triggers: Analysis loop cycle (every 40 seconds during active session)
- Preconditions: Transcript has segments, analysis window configured

## Sequence (Happy Path)

Use numbered sequence. Each step must include evidence.
For each step:

1. <what happens>
   - Evidence:
     - Code: path :: symbol/event/string
     - Docs: path :: heading/snippet
   - Notes: (only if needed)

1. Analysis loop snapshots current transcript
   - Evidence:
     - Code: server/api/ws_live_listener.py :: snapshot = list(state.transcript)
     - Docs: docs/flow-atlas-20260211.md :: INT-010 Incremental Analysis Update Flow

1. Apply 10-minute sliding window filter to transcript
   - Evidence:
     - Code: server/services/analysis_stream.py :: windowed = \_filter_window(transcript, window_seconds)
     - Docs: docs/ARCHITECTURE.md :: Cards updates every 30-60 seconds over a sliding window (default last 10 minutes)

1. \_filter_window calculates window start from max timestamp minus 600 seconds
   - Evidence:
     - Code: server/services/analysis_stream.py :: max_t1 = max(seg.get("t1", 0.0) for seg in transcript); window_start = max(0.0, max_t1 - window_seconds)
     - Docs: docs/audit/OFFLINE_CANONICAL_TRANSCRIPT_MERGE_AUDIT_2026-02-10.md :: 10-minute sliding window

1. Filter segments that overlap with the window
   - Evidence:
     - Code: server/services/analysis_stream.py :: return [seg for seg in transcript if seg.get("t0", 0.0) >= window_start or seg.get("t1", 0.0) >= window_start]

1. Run full analysis on filtered window (entities or cards)
   - Evidence:
     - Code: server/api/ws_live_listener.py :: entities = await asyncio.wait_for(asyncio.to_thread(extract_entities, snapshot), timeout=10.0)
     - Docs: docs/audit/PHASE_1C_STREAMING_BACKPRESSURE_AUDIT.md :: extract_cards() uses 10-minute sliding window

1. Send analysis updates to client
   - Evidence:
     - Code: server/api/ws_live_listener.py :: await ws_send(state, websocket, {"type": "entities_update", \*\*entities})
     - Docs: docs/flows/INT-001.md :: Send entities_update WebSocket message

## Inputs and Outputs

- Inputs: Full transcript segments with timestamps
- Outputs: Filtered transcript window, analysis results (entities/cards)
- Side effects (writes, network calls, model loads, UI state changes): WebSocket analysis updates sent

## Failure Modes

List 5â€“10 realistic failures:

- Failure: Transcript snapshot empty after filtering
  - Where detected: \_filter_window returns empty list
  - Current handling: Analysis functions handle empty input gracefully
  - User-visible outcome: Empty analysis results sent
  - Evidence: server/services/analysis_stream.py :: if not transcript: return []

- Failure: Window calculation with invalid timestamps
  - Where detected: max() over empty sequence or invalid t1 values
  - Current handling: Uses get("t1", 0.0) defaults, max(0.0, ...) clamps
  - User-visible outcome: Window starts at 0.0, includes all transcript
  - Evidence: server/services/analysis_stream.py :: max_t1 = max(seg.get("t1", 0.0) for seg in transcript); window_start = max(0.0, max_t1 - window_seconds)

- Failure: Analysis timeout on large window
  - Where detected: asyncio.wait_for timeout
  - Current handling: TimeoutError caught, cycle skipped or partial results
  - User-visible outcome: Analysis not updated for this cycle
  - Evidence: server/api/ws_live_listener.py :: except asyncio.TimeoutError: logger.warning("Entity extraction timed out...")

- Failure: Memory usage with very long transcripts
  - Where detected: list() copy operation
  - Current handling: No explicit limits, may cause OOM
  - User-visible outcome: Process crash or slowdown
  - Evidence: Assumed from unbounded transcript growth

- Failure: Window includes partial segments
  - Where detected: Overlap condition allows segments starting before window
  - Current handling: Intentional to include ongoing speech
  - User-visible outcome: Slightly larger analysis window than configured
  - Evidence: server/services/analysis_stream.py :: seg.get("t0", 0.0) >= window_start or seg.get("t1", 0.0) >= window_start

## Data and Storage

- What data is produced/consumed: Transcript segments, filtered window
- Where it is stored (DB/files/userData/etc.): In-memory only during analysis
- Retention / deletion controls (if documented): Window automatically limits to recent content

## Observability

- Logs/events emitted: None specific to windowing, analysis timeouts logged
- Correlation IDs / session IDs (if any): None
- Metrics/traces (if any): None specific
  Evidence: server/api/ws_live_listener.py :: logger.warning for timeouts

## Open Questions and Follow-up Tasks

- Questions that cannot be answered from current evidence: Why partial implementation? What would full incremental updates look like?
- Documentation gaps to fill: Rationale for 10-minute window size, performance impact of full re-analysis
- Test gaps to add (note only, no implementation): Test window boundary conditions, large transcript performance, incremental update accuracy
