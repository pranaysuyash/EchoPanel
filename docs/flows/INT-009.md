# INT-009 Embedding Generation Flow

## Summary
- Origin: Doc-only
- Status: Hypothesized
- Generate vector embeddings for transcript segments and entities using local models (EmbeddingGemma or BGE-M3) to enable semantic search in RAG pipeline.
- Boundaries crossed: Process (embedding model inference) / Storage (vector database writes)
- Primary components (coarse list): Embedding service, LanceDB vector storage, analysis pipeline

## Triggers and Preconditions
- Triggers: Session finalization or background indexing task
- Preconditions: Embedding model loaded, transcript available, vector database initialized

## Sequence (Happy Path)
Use numbered sequence. Each step must include evidence.
For each step:
1. <what happens>
   - Evidence:
     - Code: path :: symbol/event/string
     - Docs: path :: heading/snippet
   - Notes: (only if needed)

1. Session finalization triggers embedding generation
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: Embedding & Retrieval

2. Load appropriate embedding model (EmbeddingGemma or BGE-M3)
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: Default (Gemma Stack): EmbeddingGemma (300M)

3. Chunk transcript into segments suitable for embedding
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: Supports dense + sparse (multi-vector) retrieval up to 8192 tokens

4. Generate embeddings for each chunk using model inference
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: EmbeddingGemma (300M)

5. Store embeddings in LanceDB with metadata (timestamps, speaker IDs)
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: Vector Storage: LanceDB (Embedded)

6. Index entities with embeddings for entity-first search
   - Evidence:
     - Code: Hypothesized - not implemented
     - Docs: docs/RAG_PIPELINE_ARCHITECTURE.md :: + NER: Entity-first narrowing

## Inputs and Outputs
- Inputs: Transcript segments, extracted entities, session metadata
- Outputs: Vector embeddings stored in LanceDB, indexed for retrieval
- Side effects (writes, network calls, model loads, UI state changes): Embedding model loaded, vector database writes, enables semantic RAG queries

## Failure Modes
List 5â€“10 realistic failures:
- Failure: Embedding model download failed
  - Where detected: Model loading phase
  - Current handling: Hypothesized - fallback to lexical search
  - User-visible outcome: RAG uses BM25 only
  - Evidence: Hypothesized based on model lifecycle patterns

- Failure: Embedding inference timeout
  - Where detected: Model inference
  - Current handling: Hypothesized - skip chunk or reduce batch size
  - User-visible outcome: Partial embeddings generated
  - Evidence: Hypothesized from ML inference patterns

- Failure: LanceDB storage failure
  - Where detected: Database write
  - Current handling: Hypothesized - retry or mark session as non-searchable
  - User-visible outcome: Session not available for RAG queries
  - Evidence: Hypothesized based on storage failure modes

- Failure: Memory OOM during batch embedding
  - Where detected: Inference runtime
  - Current handling: Hypothesized - reduce batch size or use CPU fallback
  - User-visible outcome: Slower embedding generation
  - Evidence: Hypothesized from model memory requirements

- Failure: Corrupt embeddings data
  - Where detected: Validation step
  - Current handling: Hypothesized - regenerate or skip
  - User-visible outcome: Missing search results for some content
  - Evidence: Hypothesized from data integrity concerns

## Data and Storage
- What data is produced/consumed: Transcript text chunks, vector embeddings, metadata
- Where it is stored (DB/files/userData/etc.): LanceDB embedded database in user directory
- Retention / deletion controls (if documented): Embeddings retained with session data, deleted when session deleted

## Observability
- Logs/events emitted: Embedding generation progress, model load times, storage metrics
- Correlation IDs / session IDs (if any): Session ID in logs
- Metrics/traces (if any): Embedding latency, storage size, index build time
Evidence: Hypothesized based on existing observability patterns

## Open Questions and Follow-up Tasks
- Questions that cannot be answered from current evidence: Embedding model performance vs accuracy tradeoffs? Batch size optimization?
- Documentation gaps to fill: Model selection criteria, chunking strategy details
- Test gaps to add (note only, no implementation): Embedding quality evaluation, retrieval accuracy testing, performance benchmarking